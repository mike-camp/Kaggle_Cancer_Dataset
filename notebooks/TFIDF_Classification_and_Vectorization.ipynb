{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF classification for the Kaggle cancer dataset\n",
    "\n",
    "Using the preprocessing functions found in src/text_procesing to process the data, I run several simple logistic regression and tfidf classifications for the data.  These are simple and rely on no feature engineering so they should be taken as a benchmark for future algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>processed</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>[cyclin-depend, kinas, cdk, regul, varieti, fu...</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>[abstract, background, non-smal, cell, lung, c...</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>[abstract, background, non-smal, cell, lung, c...</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>[recent, evid, demonstr, acquir, uniparent, di...</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>[oncogen, mutat, monomer, casita, b-lineag, ly...</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class    Gene             Variation  \\\n",
       "0    1.0  FAM58A  Truncating Mutations   \n",
       "1    2.0     CBL                 W802*   \n",
       "2    2.0     CBL                 Q249E   \n",
       "3    3.0     CBL                 N454D   \n",
       "4    4.0     CBL                 L399V   \n",
       "\n",
       "                                           processed  \\\n",
       "0  [cyclin-depend, kinas, cdk, regul, varieti, fu...   \n",
       "1  [abstract, background, non-smal, cell, lung, c...   \n",
       "2  [abstract, background, non-smal, cell, lung, c...   \n",
       "3  [recent, evid, demonstr, acquir, uniparent, di...   \n",
       "4  [oncogen, mutat, monomer, casita, b-lineag, ly...   \n",
       "\n",
       "                                                text  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "1   Abstract Background  Non-small cell lung canc...  \n",
       "2   Abstract Background  Non-small cell lung canc...  \n",
       "3  Recent evidence has demonstrated that acquired...  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.merge(pd.read_pickle('../data/stem-train.pk'),pd.read_csv('../data/training_variants.csv',index_col='ID'),\n",
    "                                                                       left_index=True,right_index=True)\n",
    "df_test = pd.merge(pd.read_pickle('../data/stem-test.pk'),pd.read_csv('../data/test_variants.csv',index_col='ID'),\n",
    "                   left_index=True, right_index=True)\n",
    "df = pd.concat((df_train,df_test)).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer (Similar to bag of words)\n",
    "\n",
    "Froom this model, we get a slightly higher score then the tfidf vectorizer.  I do not understand exactly why the bag of words odes better thena  more sophisticated tfidf, but the results speak for themselves.  I will analyze the results for more than several strengths of regularization in logistic regression (Note that C is the inverse regularization strength, so smaller values of C are more regulated models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "regularization 2^-2: 1.346\n",
      "regularization 2^0: 1.166\n",
      "regularization 2^2: 1.047\n",
      "regularization 2^4: 1.018\n"
     ]
    }
   ],
   "source": [
    "def return_same(x):\n",
    "    return x\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "X,y = shuffle(df_train.processed,df_train.Class)\n",
    "print('here')\n",
    "\n",
    "for i in [2**x for x in range(-2,5,2)]:\n",
    "    pipe = Pipeline([('encoder',HashingVectorizer(analyzer=return_same)),('lr',LogisticRegression(C=i))])\n",
    "    cvs = -cross_val_score(pipe,X,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF and logistic regression\n",
    "\n",
    "We can see that TFIDF does not preform as well as just the bag of words model found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.336\n",
      "regularization 2^0: 1.137\n",
      "regularization 2^2: 1.025\n",
      "regularization 2^4: 1.029\n"
     ]
    }
   ],
   "source": [
    "for i in [2**x for x in range(-2,5,2)]:\n",
    "    pipe = Pipeline([('encoder',TfidfVectorizer(analyzer=return_same,min_df=10)),('lr',LogisticRegression(C=i))])\n",
    "    cvs = -cross_val_score(pipe,X,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tfidf on the entire dataset\n",
    "\n",
    "Lets try using TFIDF on the entire dataset and then taking the first training sets (since we have the test data, we might as well use it int the construction of our vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.350\n",
      "regularization 2^0: 1.138\n",
      "regularization 2^2: 1.012\n",
      "regularization 2^4: 1.005\n"
     ]
    }
   ],
   "source": [
    "X = TfidfVectorizer(analyzer=return_same, min_df=10).fit_transform(df.processed)[:df_train.shape[0]]\n",
    "X, y = shuffle(X, df_train.Class)\n",
    "for i in [2**x for x in range(-2,5,2)]:\n",
    "    lr = LogisticRegression(C=i)\n",
    "    cvs = -cross_val_score(lr, X, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^2: 1.015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1000)\n",
    "X = TfidfVectorizer(analyzer=return_same, min_df=10).fit_transform(df.processed)\n",
    "X_pc = pca.fit_transform(X.toarray())[:df_train.shape[0]]\n",
    "i = 4\n",
    "lr = LogisticRegression(C=i)\n",
    "X_pc, y = shuffle(X_pc, df_train.Class)\n",
    "cvs = -cross_val_score(lr, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^2: 1.044\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=500)\n",
    "X = TfidfVectorizer(analyzer=return_same, min_df=10).fit_transform(df.processed)\n",
    "X_pc = pca.fit_transform(X.toarray())[:df_train.shape[0]]\n",
    "i = 4\n",
    "lr = LogisticRegression(C=i)\n",
    "X_pc, y = shuffle(X_pc, df_train.Class)\n",
    "cvs = -cross_val_score(lr, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^2: 1.043\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=500)\n",
    "X = TfidfVectorizer(analyzer=return_same, min_df=50).fit_transform(df.processed)\n",
    "X_pc = pca.fit_transform(X.toarray())[:df_train.shape[0]]\n",
    "i = 4\n",
    "lr = LogisticRegression(C=i)\n",
    "X_pc, y = shuffle(X_pc, df_train.Class)\n",
    "cvs = -cross_val_score(lr, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^2: 1.071\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=250)\n",
    "X = TfidfVectorizer(analyzer=return_same, min_df=50).fit_transform(df.processed)\n",
    "X_pc = pca.fit_transform(X.toarray())[:df_train.shape[0]]\n",
    "i = 4\n",
    "lr = LogisticRegression(C=i)\n",
    "X_pc, y = shuffle(X_pc, df_train.Class)\n",
    "cvs = -cross_val_score(lr, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.357\n",
      "regularization 2^0: 1.172\n",
      "regularization 2^2: 1.071\n",
      "regularization 2^4: 1.054\n",
      "regularization 2^6: 1.094\n"
     ]
    }
   ],
   "source": [
    "for i in range(-2,7,2):\n",
    "    lr = LogisticRegression(C=2**i)\n",
    "    cvs = -cross_val_score(lr, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(i,cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.213\n",
      "regularization 2^0: 1.209\n",
      "regularization 2^2: 1.207\n",
      "regularization 2^4: 1.160\n",
      "regularization 2^6: 1.114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "for i in range(-2,7,2):\n",
    "    svc = SVC(probability=True,C=2**i, class_weight='balanced')\n",
    "    cvs = -cross_val_score(svc, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(i,cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.835\n",
      "regularization 2^0: 1.822\n",
      "regularization 2^2: 1.478\n",
      "regularization 2^4: 1.169\n",
      "regularization 2^6: 1.123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "for i in range(-2,7,2):\n",
    "    svc = SVC(probability=True,C=2**i, class_weight='balanced')\n",
    "    cvs = -cross_val_score(svc, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(i,cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321, 250)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators=1000,n_jobs=-1,objective='multi:softprob',num_classes=9)\n",
    "X_pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^6: 1.263\n"
     ]
    }
   ],
   "source": [
    "cvs = -cross_val_score(xgb, X_pc, y, scoring='neg_log_loss', n_jobs=-1, cv=5).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(i,cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb : 1.115\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=500,eval_metric='mlogloss',n_jobs=-1,\n",
    "                    objective='multi:softprob',num_classes=9,\n",
    "                   subsample=.5,colsample_bytree=.5,max_depth=3)\n",
    "cvs = -cross_val_score(xgb, X_pc, y, scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "print('xgb : {:.3f}'.format(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb : 1.097\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=1000,eval_metric='mlogloss',n_jobs=-1,\n",
    "                    learning_rate=.05,\n",
    "                    objective='multi:softprob',num_classes=9,\n",
    "                   subsample=.5,colsample_bytree=.5,max_depth=3)\n",
    "cvs = -cross_val_score(xgb, X_pc, y, scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "print('xgb : {:.3f}'.format(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb : 0.978\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=1000,eval_metric='mlogloss',n_jobs=-1,\n",
    "                    learning_rate=.02,\n",
    "                    objective='multi:softprob',num_classes=9,\n",
    "                   subsample=.5,colsample_bytree=.5,max_depth=3)\n",
    "cvs = -cross_val_score(xgb, X_pc, y, scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "print('xgb : {:.3f}'.format(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=500)\n",
    "X = TfidfVectorizer(analyzer=return_same, min_df=10).fit_transform(df.processed)\n",
    "X_pc = pca.fit_transform(X.toarray())[:df_train.shape[0]]\n",
    "X_pc, y = shuffle(X_pc, df_train.Class)\n",
    "xgb = XGBClassifier(n_estimators=2000,eval_metric='mlogloss',n_jobs=-1,\n",
    "                    learning_rate=.01,\n",
    "                    objective='multi:softprob',num_classes=9,\n",
    "                   subsample=.5,colsample_bytree=.5,max_depth=3)\n",
    "cvs = -cross_val_score(xgb, X_pc, y, scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "print('xgb : {:.3f}'.format(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "cvs = -cross_val_score(xgb, X_pc, y, scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "print('xgb : {:.3f}'.format(cvs))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
