{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF classification for the Kaggle cancer dataset\n",
    "\n",
    "Using the preprocessing functions found in src/text_procesing to process the data, I run several simple logistic regression and tfidf classifications for the data.  These are simple and rely on no feature engineering so they should be taken as a benchmark for future algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = pd.read_csv('../data/training_variants.csv')\n",
    "x = pd.read_pickle('../data/local_pk.pk')\n",
    "x_new = pd.read_pickle('../data/new_tokens.pk')\n",
    "y = pd.read_csv('../data/training_variants.csv',index_col=0).Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "      <td>[cyclin-depend, kinas, cdk, regul, varieti, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>[abstract, background, non-smal, cell, lung, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>[abstract, background, non-smal, cell, lung, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "      <td>[recent, evid, demonstr, acquir, uniparent, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "      <td>[oncogen, mutat, monomer, casita, b-lineag, ly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
       "1   Abstract Background  Non-small cell lung canc...   \n",
       "2   Abstract Background  Non-small cell lung canc...   \n",
       "3  Recent evidence has demonstrated that acquired...   \n",
       "4  Oncogenic mutations in the monomeric Casitas B...   \n",
       "\n",
       "                                           processed  \n",
       "0  [cyclin-depend, kinas, cdk, regul, varieti, fu...  \n",
       "1  [abstract, background, non-smal, cell, lung, c...  \n",
       "2  [abstract, background, non-smal, cell, lung, c...  \n",
       "3  [recent, evid, demonstr, acquir, uniparent, di...  \n",
       "4  [oncogen, mutat, monomer, casita, b-lineag, ly...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer (Similar to bag of words)\n",
    "\n",
    "Froom this model, we get a slightly higher score then the tfidf vectorizer.  I do not understand exactly why the bag of words odes better thena  more sophisticated tfidf, but the results speak for themselves.  I will analyze the results for more than several strengths of regularization in logistic regression (Note that C is the inverse regularization strength, so smaller values of C are more regulated models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.531\n",
      "regularization 2^-1: 1.502\n",
      "regularization 2^0: 1.501\n",
      "regularization 2^1: 1.535\n",
      "regularization 2^2: 1.606\n",
      "\n",
      "\n",
      "\n",
      "regularization 2^-2: 1.527\n",
      "regularization 2^-1: 1.498\n",
      "regularization 2^0: 1.498\n",
      "regularization 2^1: 1.531\n",
      "regularization 2^2: 1.603\n"
     ]
    }
   ],
   "source": [
    "def return_same(x):\n",
    "    return x\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "for i in [2**x for x in range(-2,3)]:\n",
    "    pipe = Pipeline([('encoder',HashingVectorizer(analyzer=return_same)),('lr',LogisticRegression(C=i))])\n",
    "    cvs = -cross_val_score(pipe,x.text,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))\n",
    "print('\\n\\n')\n",
    "for i in [2**x for x in range(-2,3)]:\n",
    "    pipe = Pipeline([('encoder',HashingVectorizer(analyzer=return_same)),('lr',LogisticRegression(C=i))])\n",
    "    cvs = -cross_val_score(pipe,x_new.processed,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF and logistic regression\n",
    "\n",
    "We can see that TFIDF does not preform as well as just the bag of words model found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^-2: 1.618\n",
      "regularization 2^-1: 1.597\n",
      "regularization 2^0: 1.605\n",
      "regularization 2^1: 1.650\n",
      "regularization 2^2: 1.734\n",
      "regularization 2^-2: 1.615\n",
      "regularization 2^-1: 1.594\n",
      "regularization 2^0: 1.603\n",
      "regularization 2^1: 1.648\n",
      "regularization 2^2: 1.733\n"
     ]
    }
   ],
   "source": [
    "for i in [2**x for x in range(-2,3)]:\n",
    "    pipe = Pipeline([('encoder',TfidfVectorizer(analyzer=return_same)),('lr',LogisticRegression(C=i))])\n",
    "    cvs = -cross_val_score(pipe,x.text,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))\n",
    "    \n",
    "    \n",
    "for i in [2**x for x in range(-2,3)]:\n",
    "    pipe = Pipeline([('encoder',TfidfVectorizer(analyzer=return_same)),('lr',LogisticRegression(C=i))])\n",
    "    cvs = -cross_val_score(pipe,x_new.processed,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "    print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization 2^2: 1.754\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_new.processed,y)\n",
    "pipe = Pipeline([('encoder',HashingVectorizer(analyzer=return_same)),\n",
    "                 ('svc',SVC(kernel='linear',probability=True))])\n",
    "cvs = -cross_val_score(pipe,x_new.processed,y,scoring='neg_log_loss',n_jobs=-1,cv=3).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('encoder',HashingVectorizer(analyzer=return_same)),\n",
    "                 ('svc',SVC(kernel='linear',probability=True,class_weight='balanced'))])\n",
    "cvs = -cross_val_score(pipe,x_new.processed,y,scoring='neg_log_loss',n_jobs=-1,cv=5).mean()\n",
    "print('regularization 2^{:d}: {:.3f}'.format(int(np.log2(i)),cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
